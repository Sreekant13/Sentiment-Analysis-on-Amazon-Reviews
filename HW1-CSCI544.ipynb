{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91912\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91912\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\91912\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91912\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gzip\n",
    "import requests\n",
    "from io import BytesIO, TextIOWrapper\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4\n",
    "#! pip install scikit-learn\n",
    "#! pip install contractions\n",
    "#Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "3          US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
       "4          US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3  AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4  Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          1.0    N                 Y   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "3           1            2.0          3.0    N                 Y   \n",
       "4           4            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "3  and the shredder was dirty and the bin was par...   \n",
       "4                                         Four Stars   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3  Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                    Gorgeous colors and easy to use  2015-08-31  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing URL of the dataset in url\n",
    "url = \"https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
    "\n",
    "# Here we will fetch the dataset\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  #  Had to use this to ensure that the request was successful\n",
    "\n",
    "# Now we need to decompress the content to use it for further tasks\n",
    "with gzip.GzipFile(fileobj=BytesIO(response.content)) as gz:\n",
    "    amazon_review_data = pd.read_csv(\n",
    "        TextIOWrapper(gz, encoding='utf-8'),  # we set this explicitly to UTF-8 encoding because it was giving error for some values which was not in the proper format\n",
    "        sep='\\t',\n",
    "        on_bad_lines='skip',  # Here we will skip problematic lines\n",
    "        low_memory=False      # Lastly we do this to improve performance for large datasets\n",
    "    )\n",
    "\n",
    "#Finally we will print few lines to make sure that the data has been successfully loaded\n",
    "amazon_review_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews: 2001052, Negative reviews: 445348, Neutral reviews: 193680\n",
      "Training size: 160000\n",
      "Testing size: 40000\n",
      "Class distribution in training: label\n",
      "1.0    80007\n",
      "0.0    79993\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# A small function to assign star rating to binary labels\n",
    "def assign_label(star_rating):\n",
    "    if star_rating > 3:\n",
    "        return 1\n",
    "    elif star_rating <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# we will filter relevant columns and will keep only Reviews and Ratings columns and drop the rest of the columns\n",
    "amazon_review_data = amazon_review_data[['review_body', 'star_rating']].dropna()\n",
    "\n",
    "# We need to make sure that star_rating is numeric for further parts in code\n",
    "amazon_review_data['star_rating'] = pd.to_numeric(amazon_review_data['star_rating'], errors='coerce')\n",
    "\n",
    "# Print statistics for the three classes: positive, negative, and neutral reviews\n",
    "positive_count = (amazon_review_data['star_rating'] > 3).sum()\n",
    "negative_count = (amazon_review_data['star_rating'] <= 2).sum()\n",
    "neutral_count = (amazon_review_data['star_rating'] == 3).sum()\n",
    "\n",
    "# Print the counts for each class in the requested format\n",
    "print(f\"Positive reviews: {positive_count}, Negative reviews: {negative_count}, Neutral reviews: {neutral_count}\")\n",
    "\n",
    "# Now we map ratings to binary labels\n",
    "amazon_review_data['label'] = amazon_review_data['star_rating'].apply(assign_label)\n",
    "\n",
    "# As sid in the assignment, we need to drop neutral reviews\n",
    "amazon_review_data = amazon_review_data.dropna(subset=['label'])\n",
    "\n",
    "# We will downsample the dataset to 100,000 positive and negative reviews according to the assignment\n",
    "# Also we will be using random state = 42 such that we can get the consistent results\n",
    "# Assign 1 for positive reviews and 0 for negative reviews\n",
    "positive_reviews = amazon_review_data[amazon_review_data['label'] == 1].sample(100000, random_state=42) \n",
    "negative_reviews = amazon_review_data[amazon_review_data['label'] == 0].sample(100000, random_state=42)\n",
    "\n",
    "# We will now combine the downsampled amazon_review_data\n",
    "balanced_data = pd.concat([positive_reviews, negative_reviews])\n",
    "\n",
    "# Finally the main task where we will split into training and testing datasets\n",
    "train_data, test_data = train_test_split(balanced_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Lastly, we print dataset statistics as asked\n",
    "print(\"Training size:\", len(train_data))\n",
    "print(\"Testing size:\", len(test_data))\n",
    "print(\"Class distribution in training:\", train_data['label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert the all reviews into the lower case.\n",
    "- remove the HTML and URLs from the reviews\n",
    "- remove non-alphabetical characters\n",
    "- remove extra spaces\n",
    "- perform contractions on the reviews, e.g., won’t → will not. Include as many contractions in English that you can think "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\",\n",
    "    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\", \"don't\": \"do not\", \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\", \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\", \"wouldn't\": \"would not\", \"shouldn't\": \"should not\",\n",
    "    \"couldn't\": \"could not\", \"mightn't\": \"might not\", \"mustn't\": \"must not\",\n",
    "    \"let's\": \"let us\", \"that's\": \"that is\", \"what's\": \"what is\",\n",
    "    \"who's\": \"who is\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "    \"how's\": \"how is\", \"where's\": \"where is\", \"why's\": \"why is\",\n",
    "    \"when's\": \"when is\", \"weren't\": \"were not\", \"could've\": \"could have\",\n",
    "    \"should've\": \"should have\", \"would've\": \"would have\", \"might've\": \"might have\",\n",
    "    \"must've\": \"must have\", \"we've\": \"we have\", \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\", \"who've\": \"who have\", \"i've\": \"i have\",\n",
    "    \"hasn't\": \"has not\", \"you'll\": \"you will\", \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\", \"it'll\": \"it will\", \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\", \"i'll\": \"i will\", \"that'll\": \"that will\",\n",
    "    \"there'll\": \"there will\", \"who'll\": \"who will\", \"what'll\": \"what will\",\n",
    "    \"won't\": \"will not\", \"shan't\": \"shall not\", \"who'd\": \"who would\",\n",
    "    \"it'd\": \"it would\", \"we'd\": \"we would\", \"they'd\": \"they would\",\n",
    "    \"you'd\": \"you would\", \"she'd\": \"she would\", \"he'd\": \"he would\",\n",
    "    \"i'd\": \"i would\", \"they're\": \"they are\", \"we're\": \"we are\",\n",
    "    \"you're\": \"you are\", \"i'm\": \"i am\", \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\", \"it's\": \"it is\", \"ain't\": \"is not\",\n",
    "    \"y'all\": \"you all\", \"gonna\": \"going to\", \"wanna\": \"want to\",\n",
    "    \"gotta\": \"got to\", \"lemme\": \"let me\", \"gimme\": \"give me\",\n",
    "    \"dunno\": \"do not know\", \"outta\": \"out of\", \"sorta\": \"sort of\",\n",
    "    \"kinda\": \"kind of\", \"oughta\": \"ought to\", \"coulda\": \"could have\",\n",
    "    \"woulda\": \"would have\", \"shoulda\": \"should have\", \"how'd\": \"how did\",\n",
    "    \"why'd\": \"why did\", \"where'd\": \"where did\", \"when'd\": \"when did\",\n",
    "    \"y'know\": \"you know\", \"c'mon\": \"come on\", \"how're\": \"how are\",\n",
    "    \"what're\": \"what are\", \"who're\": \"who are\", \"where're\": \"where are\",\n",
    "    \"when're\": \"when are\", \"why're\": \"why are\", \"there're\": \"there are\",\n",
    "    \"that'd\": \"that would\", \"this'll\": \"this will\", \"it'll've\": \"it will have\",\n",
    "    \"we'll've\": \"we will have\", \"who'll've\": \"who will have\", \n",
    "    \"it'd've\": \"it would have\", \"nothin'\": \"nothing\", \"somethin'\": \"something\",\n",
    "    \"everythin'\": \"everything\", \"givin'\": \"giving\", \"movin'\": \"moving\",\n",
    "    \"y'all've\": \"you all have\", \"y'all'd\": \"you all would\", \n",
    "    \"ain'tcha\": \"are not you\", \"didn'tcha\": \"did not you\",\n",
    "    \"ya'll\": \"you all\", \"ain'tcha\": \"are not you\", \"mightn't've\": \"might not have\",\n",
    "    \"mustn't've\": \"must not have\", \"shouldn't've\": \"should not have\",\n",
    "    \"you'd've\": \"you would have\", \"there'd've\": \"there would have\",\n",
    "    \"who'd've\": \"who would have\", \"what'd've\": \"what would have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Function to expand contractions based on whatever is needed\n",
    "def expand_contractions(text):\n",
    "    words = text.split()\n",
    "    expanded_words = []\n",
    "    for word in words:\n",
    "        # We will check if the word is in the contractions dictionary\n",
    "        if word in contractions_dict:\n",
    "            # Here we replace the word with its expanded form\n",
    "            expanded_words.append(contractions_dict[word])\n",
    "        else:\n",
    "            # Keep the word as is\n",
    "            expanded_words.append(word)\n",
    "    return \" \".join(expanded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length before cleaning: 317.4268625\n",
      "Average length after cleaning: 299.67749375\n"
     ]
    }
   ],
   "source": [
    "# Updated cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # We will convert text to lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # We will Remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # We will remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # We will remove non-alphabetical characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # We will remove extra spaces\n",
    "    text = expand_contractions(text)  # We will expand contractions manually\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to training and testing datasets\n",
    "train_data['cleaned_review'] = train_data['review_body'].apply(clean_text)\n",
    "test_data['cleaned_review'] = test_data['review_body'].apply(clean_text)\n",
    "\n",
    "# Print average length before and after cleaning\n",
    "print(\"Average length before cleaning:\", train_data['review_body'].str.len().mean())\n",
    "print(\"Average length after cleaning:\", train_data['cleaned_review'].str.len().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do disappointed that the chocolate was melted and stuck to the plastic impossible to remove perhaps shipping'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Helper Cell for debugging\n",
    "train_data.head()\n",
    "train_data['cleaned_review'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review: Do disappointed that the chocolate was melted and stuck to the plastic. Impossible to remove. Perhaps shipping?\n",
      "Cleaned Review (Function): do disappointed that the chocolate was melted and stuck to the plastic impossible to remove perhaps shipping\n",
      "Cleaned Review (Loaded): do disappointed that the chocolate was melted and stuck to the plastic impossible to remove perhaps shipping\n"
     ]
    }
   ],
   "source": [
    "#Helper Cell for debugging and checking if the cleaning is happening properly or not\n",
    "sample_review = train_data['review_body'].iloc[0]\n",
    "cleaned_review_1 = clean_text(sample_review)\n",
    "print(\"Original Review:\", sample_review)\n",
    "print(\"Cleaned Review (Function):\", cleaned_review_1)\n",
    "print(\"Cleaned Review (Loaded):\", train_data['cleaned_review'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # type: ignore\n",
    "\n",
    "# We will set and define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# This function is to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text)  # Here we will tokenize the text\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Here we will filter out stop words\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer # type: ignore\n",
    "# We will initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]  # Here we will lemmatize each token\n",
    "    return ' '.join(lemmatized)  # Here we will join tokens back into a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print three sample reviews before and after data cleaning + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 52190:\n",
      "Before preprocessing: this was detected as a cartridge that was not accepted by the lexmark printer it was made for had to get another that had a chip that the printer would accept was without a printer for weeks\n",
      "After preprocessing: detected cartridge accepted lexmark printer made get another chip printer would accept without printer week\n",
      "--------------------------------------------------\n",
      "Sample 143446:\n",
      "Before preprocessing: the color cartridges work just fine for me so far but the black cartridges do not work at all the quality is terrible and looks blurredwhen it prints also greatly reduced the quality of the color cartridgethe color was fine until i used the compatible black cartridge then the color all but stopped working ill keep these to use if im in a bind but i wont reorder\n",
      "After preprocessing: color cartridge work fine far black cartridge work quality terrible look blurredwhen print also greatly reduced quality color cartridgethe color fine used compatible black cartridge color stopped working ill keep use im bind wont reorder\n",
      "--------------------------------------------------\n",
      "Sample 103578:\n",
      "Before preprocessing: i havent used it much but so far it works great only one time did i have to reset it to my wireless router\n",
      "After preprocessing: havent used much far work great one time reset wireless router\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We need to ensure that all values in 'cleaned_review' are strings and replace NaN values with an empty string\n",
    "train_data['cleaned_review'] = train_data['cleaned_review'].fillna(\"\").astype(str)\n",
    "test_data['cleaned_review'] = test_data['cleaned_review'].fillna(\"\").astype(str)\n",
    "\n",
    "# This is the preprocessing function where we will call the other functions(like the remove stop word function and lemmatize token function) for preprocessing our data\n",
    "def preprocess_text(text):\n",
    "    tokens = remove_stop_words(text)  # Remove stop words\n",
    "    processed_text = lemmatize_tokens(tokens)  # Perform lemmatization\n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "train_data['preprocessed_review'] = train_data['cleaned_review'].apply(preprocess_text)\n",
    "test_data['preprocessed_review'] = test_data['cleaned_review'].apply(preprocess_text)\n",
    "\n",
    "# Print three random samples before and after preprocessing\n",
    "sample_indices = random.sample(range(len(train_data)), 3)  # Select 3 random indices\n",
    "for i in sample_indices:\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(\"Before preprocessing:\", train_data['cleaned_review'].iloc[i])\n",
    "    print(\"After preprocessing:\", train_data['preprocessed_review'].iloc[i])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length before preprocessing: 299.67749375\n",
      "Average length after preprocessing: 190.75154375\n"
     ]
    }
   ],
   "source": [
    "# Handle NaN values in 'preprocessed_review'\n",
    "train_data['preprocessed_review'] = train_data['preprocessed_review'].fillna(\"\")\n",
    "test_data['preprocessed_review'] = test_data['preprocessed_review'].fillna(\"\")\n",
    "\n",
    "# Printing the average length after preprocessing the data\n",
    "print(\"Average length before preprocessing:\", train_data['cleaned_review'].str.len().mean())\n",
    "print(\"Average length after preprocessing:\", train_data['preprocessed_review'].str.len().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# We will Extract TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf.fit_transform(train_data['preprocessed_review']).toarray()\n",
    "X_test = tfidf.transform(test_data['preprocessed_review']).toarray()\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Metrics:\n",
      "Accuracy Score Train: 0.8316375\n",
      "Precision Score Train: 0.7595949713838478\n",
      "Recall Score Train: 0.970440086492432\n",
      "F1 Score Train: 0.8521693319138194\n",
      "Accuracy Score Test: 0.821125\n",
      "Precision Score Test: 0.7494365430947385\n",
      "Recall Score Test: 0.9646376231681089\n",
      "F1 Score Test: 0.8435278938045356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Training Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Predictions being made here\n",
    "y_train_pred = perceptron.predict(X_train)\n",
    "y_test_pred = perceptron.predict(X_test)\n",
    "\n",
    "# Printing Metrics\n",
    "print(\"Perceptron Metrics:\")\n",
    "print(\"Accuracy Score Train:\",accuracy_score(y_train, y_train_pred))\n",
    "print(\"Precision Score Train:\",precision_score(y_train, y_train_pred))\n",
    "print(\"Recall Score Train:\",recall_score(y_train, y_train_pred))\n",
    "print(\"F1 Score Train:\",f1_score(y_train, y_train_pred))\n",
    "print(\"Accuracy Score Test:\",accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision Score Test:\",precision_score(y_test, y_test_pred))\n",
    "print(\"Recall Score Test:\",recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score Test:\",f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVM Metrics:\n",
      "Accuracy Score Train: 0.9070875\n",
      "Precision Score Train: 0.9089562171188931\n",
      "Recall Score Train: 0.9048208281775345\n",
      "F1 Score Train: 0.9068838083307235\n",
      "Accuracy Score Test: 0.893175\n",
      "Precision Score Test: 0.8927643413951629\n",
      "Recall Score Test: 0.8936127644675637\n",
      "F1 Score Test: 0.893188351456068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Training LinearSVC\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "# Predictions being made here\n",
    "y_train_pred = linear_svc.predict(X_train)\n",
    "y_test_pred = linear_svc.predict(X_test)\n",
    "\n",
    "# Printing Metrics\n",
    "print(\"LinearSVM Metrics:\")\n",
    "print(\"Accuracy Score Train:\",accuracy_score(y_train, y_train_pred))\n",
    "print(\"Precision Score Train:\",precision_score(y_train, y_train_pred))\n",
    "print(\"Recall Score Train:\",recall_score(y_train, y_train_pred))\n",
    "print(\"F1 Score Train:\",f1_score(y_train, y_train_pred))\n",
    "print(\"Accuracy Score Test:\",accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision Score Test:\",precision_score(y_test, y_test_pred))\n",
    "print(\"Recall Score Test:\",recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score Test:\",f1_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy Score Train: 0.902725\n",
      "Precision Score Train: 0.9053579740593038\n",
      "Recall Score Train: 0.8994962940742685\n",
      "F1 Score Train: 0.9024176154887897\n",
      "Accuracy Score Test: 0.893825\n",
      "Precision Score Test: 0.8944784046497645\n",
      "Recall Score Test: 0.8929125193817836\n",
      "F1 Score Test: 0.8936947761007233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Training Logistic Regression\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Predictions being made here\n",
    "y_train_pred = logistic_regression.predict(X_train)\n",
    "y_test_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# Printing Metrics\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(\"Accuracy Score Train:\",accuracy_score(y_train, y_train_pred))\n",
    "print(\"Precision Score Train:\",precision_score(y_train, y_train_pred))\n",
    "print(\"Recall Score Train:\",recall_score(y_train, y_train_pred))\n",
    "print(\"F1 Score Train:\",f1_score(y_train, y_train_pred))\n",
    "print(\"Accuracy Score Test:\",accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision Score Test:\",precision_score(y_test, y_test_pred))\n",
    "print(\"Recall Score Test:\",recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score Test:\",f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Metrics:\n",
      "Accuracy Score Train: 0.8668125\n",
      "Precision Score Train: 0.8666866574209429\n",
      "Recall Score Train: 0.8670116364818078\n",
      "F1 Score Train: 0.8668491164929645\n",
      "Accuracy Score Test: 0.861775\n",
      "Precision Score Test: 0.8601952385695787\n",
      "Recall Score Test: 0.8638523483219127\n",
      "F1 Score Test: 0.8620199146514936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Predictions being made here\n",
    "y_train_pred = naive_bayes.predict(X_train)\n",
    "y_test_pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# Printing Metrics\n",
    "print(\"Naive Bayes Metrics:\")\n",
    "print(\"Accuracy Score Train:\",accuracy_score(y_train, y_train_pred))\n",
    "print(\"Precision Score Train:\",precision_score(y_train, y_train_pred))\n",
    "print(\"Recall Score Train:\",recall_score(y_train, y_train_pred))\n",
    "print(\"F1 Score Train:\",f1_score(y_train, y_train_pred))\n",
    "print(\"Accuracy Score Test:\",accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision Score Test:\",precision_score(y_test, y_test_pred))\n",
    "print(\"Recall Score Test:\",recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score Test:\",f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron model:\n",
    "#### Training Accuracy: ~83% \n",
    "#### Testing Accuracy: ~82%\n",
    "#### This modal has a high recall (97% on both sets) indicates the model is good at identifying positive samples.\n",
    "#### It's Precision (75%) is lower which suggests that there are some false positives.\n",
    "#### It has balanced F1 scores (~85%) highlighting reasonable overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM:\n",
    "#### Training Accuracy: 90.5%\n",
    "#### Testing Accuracy: 89.0%\n",
    "#### This model has excellent balance between precision (90%) and recall (89%) on both sets.\n",
    "#### It also has higher F1 scores (~89%) which suggests a robust and well-generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:\n",
    "#### Training Accuracy: 90.2%\n",
    "#### Testing Accuracy: 89.3%\n",
    "#### This model's performance is nearly identical to Linear SVM, with strong precision (~89%) and balanced recall.\n",
    "#### It has a slightly lower F1 score compared to Linear SVM but still competitive when compared to all 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes:\n",
    "#### Training Accuracy: 86.6%\n",
    "#### Testing Accuracy: 86.0%\n",
    "#### This model has a decent performance with balanced precision and recall (~86%).\n",
    "#### It has a lower accuracy compared to SVM and Logistic Regression but still reasonable for simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Efficiency\n",
    "##### Best Model in this case is Linear SVM for its high accuracy, precision, recall, and well-balanced performance.\n",
    "##### Naive Bayes is simpler and less computationally expensive but slightly less accurate.\n",
    "##### Perceptron model is adequate but less reliable due to false positives.\n",
    "##### Logistic Regression is excellent as a close competitor to Linear SVM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
